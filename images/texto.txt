In order to support the good performance that we could achieve and the results of selecting relevant features to solve the problem, we will show the visualization of our data. We selected two main algorithms to achieve this: PCA (Principal Component Analysis) and t-SNE (t-Distributed Stochastic Neighbor Embedding). They are used in this work to tackle the problem of curse of dimensionality in our data. Even if we have a low number of features if we consider real-world problems with high dimensional data (e.g. image classification), it is important to see the relationships between them to try to remove noise and difficulties for the classifier. This part of the work aims to have a better understading of our data and to support the idea to make it less complex.

PCA is a technique of dimension reduction but can also be used to visualize the data. We can select the number of Principal Components to be considered as two in order to visualize the data. However, PCA only captures linear relationships and two principal components might not be enough to get the percentage of variation that we want to be captured in the abridged dataset. In order to improve the vizualization with PCA, we considered different number of features. They were selected in the same order as the previous part of the work (where we select the relevant features).

IMAGE merged_PCA 
LEGENDA:

We could not find a nice structure behind the data. It might be because PCA only captures linear relationships and our dataset can have non-linear associations.

t-SNE is also a technique for dimensionaly reduction but it is able to capture non-linear relationships. Specifically, it is well suited for visualization. It does not preserve distances nor densities. However, it aims to preserve nearest-neighbors and it can lead to good visualizations.

IMAGE TSNE_OFFICIAL.jpeg
LEGENDA DA FOTO: T-SNE algorithm with paramenters: learning_rate=200.0, n_iter=1500, early_exaggeration=5.0, perplexity=100. It uses the top 8 features based on selectKbest method using "chi2" as a score function. 

IMAGE TSNE_OFFICIAL 
LEGENDA:

As we can see, t-SNE was able to represent better our data and it could give us good insights about the classification. We can not define a precisely number of clusters, however we can see different regions based on three sets: {class 3}, {class 2} and {classes 1 and 0}. The set of classes 1 and 0 are physically difficult to classify and they are blurried in most of the regions in the image. Another important thing to see is when we are using the top 2 or 3 features, it seems to have a direction that increases our classification. However, in the middle of those transitions, it is hard for a classifier to separate them. It happens mostly because we need more features to separate those more-specific cases. We can see in the final image (which had the hyperparameters of t-sne optimized) using the top 8 features (which makes the classifier have an accuracy greater than 90%) that appears to have different directions to increase the classification and when the region of 0 and 1 finishes, it ussually leads to class 2 and then class 3. But using those features we could separate those aforementioned more-specific cases better.

Outra parada que pode comentar aqui é que aumentando o numero de features, o tsne começa a falhar na representação... Isso pode ser por falta de otimização dos hyperparametros, mas também (o que é interessante) é que talvez algumas dessas features podem ser removidas e mesmo assim a gente tem a mesma acurácia, talvez fazendo redes neurais mais complexas ou tentando outros métodos (talvez emsemble learning)


